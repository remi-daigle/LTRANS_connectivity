---
title: "Tutorial to extract connectivity matrices from raw LTRANS output"
output: 
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float: yes
  github_document: 
    pandoc_args: [ 
      "--output=README.md" 
    ] 
---
Before we begin, if you want to follow along, this tutorial assumes you have already installed [R](http://www.r-project.org/) and [RStudio](http://www.rstudio.com/). Additionally, you will need the data contained in the project folder. You can go to the GitHub [repository](https://github.com/remi-daigle/LTRANS_connectivity) to download (or fork, etc) the whole project folder including the R markdown file used to generate this website, or you can [click here](https://github.com/remi-daigle/LTRANS_connectivity/archive/gh-pages.zip) to download the project folder as a zipped file.

I recommend opening using RStudio and opening the `LTRANS_connectivity.Rproj` inside the project directory, since that sets the R working directory to wherever you have downloaded the project. That way R will know where to find the data!

Let's start by loading our packages, if you haven't installed these previously, you will need to install them manually (e.g. `install.packages("tidyverse")`)
```{r loading packages,message=FALSE}
require(data.table)
require(tidyverse)
require(readr)
require(rgdal)
require(rgeos)
require(maptools)
```

Processing all the data in one pass will make your computer burst into flames, so lets first pick a `year` and a `pld`.
```{r setting year and pld}
year <- 1999
pld <- 120
```

# Loading data
## Release Grid
Let's load up and plot the grid we used to generate the particle releases. It's a hexagonal grid that DFO Maritimes regions is using in their MPA planning MARXAN analysis. I've taken the liberty to expand that grid beyond the DFO Maritimes region to cover the entire model domain from 0-250 m depth.

We don't really need this, but might be useful for visualization and indexing.
```{r loading grid}
grid250 <- readOGR("data/shapefiles","grid250")
plot(grid250)
```

## Release locations
Let's load up the release locations files that I generated using the above grid. First, list the files
```{r load release locations}
# list the files
filenames <- list.files(path="data/LTRANS_input", pattern="rl.", full.names=TRUE,recursive=T)

# load all files into a list, read_csv is much faster than read.csv
rllist <- lapply(filenames, read_csv,
                 col_names = c("long0","lat0","Z0","delay","site0"),
                 col_types = cols("d","d","i","i","i")
)

# set the names of the items in the list, so that you know which file it came from
rllist <- setNames(rllist,filenames)

# rbind the list
rl <- rbindlist(rllist, idcol="filename")

# The `site0` column was not set in the release location file. But in our case (you'll have to trust me) they correspond to the `UNIT_ID` in `grid250`
rl$site0 <- grid250$UNIT_ID
head(rl)
```

The filename is a little clunky to work with. We just need the number to match with the directory structure I set for the raw data (more on this below).
```{r define rl$bin}
rl$bin <- gsub(".*rl_|.txt.*", "",rl$filename)
head(rl)
```

## LTRANS data
Before we start, I need to explain how I've organized the output in different directories indicating the `year`, `day`, and `bin`. However, I've preceded `year` with the letter `E` to indicate east, so I don't mix up my coasts! So if you look at the files in [`data/LTRANS_output/E1999/152/1/`](https://github.com/remi-daigle/LTRANS_connectivity/tree/master/data/LTRANS_output/E1999/152/1); That is the output of one model run which used the east coast model, using current velocities from the year 1999. In this case particles were released on the 152nd day of the year (June 1st). Lastly the `bin` number `1` relates to which release location file was used, in this case it was [rl_1.txt](https://github.com/remi-daigle/LTRANS_connectivity/blob/master/data/LTRANS_input/rl_1.txt). This type of directory structure is not typical of LTRANS output, but rather how I've chosen to organize my data.

Getting back to the files that are produced from one model run, I ignore the `endfile.csv`, and focus on the `para    XXXX.csv` files. I ran the particle tracking for 120 days and you'll see that the files are all sequentially numbered 1002 to 1121 (and that there are 120 of them). For reasons beyond my understanding, the file that has the positions of the larvae at the end of 'day 1' is `para    1002.csv', 'day 2' is `para    1003.csv', and so on until 'day 120' is `para    1121.csv'
```{r load LTRANS output}
# list the particle tracking files for that particular year and pld
filenames <- list.files(path=paste0("data/LTRANS_output/E",year), pattern=glob2rx(paste0("*para    1",formatC(pld+1, width = 3, format = "d", flag = "0"),"*")), full.names=TRUE,recursive=T)

# load all files into a list, read_csv is much faster than read.csv
datalist <- lapply(filenames, read_csv,
                 col_names = c("long","lat","Z","Out","site"),
                 col_types = cols("d","d","d","i","i")
)

# set the names of the items in the list, so that you know which file it came from
datalist <- setNames(datalist,filenames)

# rbind the list
dataset <- rbindlist(datalist, idcol="filename")
dataset$site <- NA
rm(datalist)

# extract all the useful information out of filename
dataset <- dataset %>%
    mutate(temp=substr(filename,20,nchar(filename))) %>%
    separate(temp,c("temp_type_year","rday","bin","time"),"/",convert=TRUE) %>% 
    separate(temp_type_year,c("type","year"),sep=1,convert=TRUE) %>% 
    mutate(time=as.integer(substr(time,9,12)))
    
# link release data to output by bin, not elegant, but it works!

for(i in unique(dataset$bin)){
    x <- rl$bin==i
    y <- dataset$bin==i
    dataset$long0[y] <- rl$long0[x]
    dataset$lat0[y] <- rl$lat0[x]
    dataset$Z0[y] <- rl$Z0[x]
    dataset$delay[y] <- rl$delay[x]
    dataset$site0[y] <- rl$site0[x]
}
head(dataset)
```

# Calculating connectivity
## Determine settlement location
Now, we know what site we released each particle from (`site0`) and the latitude/longitude of where each particle was after a pld of `r pld` days. However, we don't know the `site` that corresponds to the latitude/longitude. We could fire up R's GIS type tools (e.g. `rgeos::gContains`), but we don't need anything that fancy for this type a grid. We only need to know which release location (which are the center of each grid hexagon). 

Now with this example `dataset` we could likely calculate all of
```{r define dataset$site} 
dataset$site <- NA

n <- 1000
for(i in seq(1,nrow(dataset),n)){
    i2 <- i+n-1
    if(i2>nrow(dataset)) i2 <- nrow(dataset)
    
    dataset$site[i:i2] <- apply(dataset[i:i2,],1,
                                function(x) which.min(
                                    abs(
                                        (rl$lat0-as.numeric(x[names(dataset)=="lat"]))
                                    )+
                                        abs(
                                            (rl$long0-as.numeric(x[names(dataset)=="long"]))
            )))
}
head(dataset)    
```

## Generate sampling grid
Now the release grid has `r length(grid250)` cells. If we tried to calculate a matrix using those, the resulting matrix would be over 45 GB, big problem! Don't believe me? Try it!
```{r demonstrate error, eval=FALSE}
matrix(0,ncol=length(grid250),nrow=length(grid250))
```

Anyway, the point is, we need a bigger cell size grid so we can simplify this computationally.
```{r generate sampling grid}
uniongrid <- unionSpatialPolygons(grid250,rep(1,length(grid250)))

proj <- "+proj=utm +zone=20 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"
uniongrid <- spTransform(uniongrid,CRS(proj))

bb <- Spatial(bbox(gBuffer(uniongrid,width=100000)), proj4string = CRS(proj))
             
biggridbb <- HexPoints2SpatialPolygons(spsample(bb, type="hexagonal", n=500))
plot(biggridbb)

plot(uniongrid,add=T,col='blue')
```

## Grid tyding
So, now we have an OK grid we can use for demonstration purposes, but let's remove the cells that don't overlap with `grid250` and let's create a key so that we can figure our which cells from `grid250` overlap with which cells in `biggrid`
```{r cleanup new grid}
biggrid <- biggridbb[gIntersects(biggridbb,uniongrid,byid=T)[1,],]
biggrid <- spChFIDs(biggrid,as.character(1:length(biggrid)))
plot(biggrid)
plot(uniongrid,add=T,col='blue')

# before we keep going let's change the projection on `biggrid` back to what we have for `grid250`
biggrid <- spTransform(biggrid,CRS(proj4string(grid250)))
```

Make a key
```{r making a key}
key <- gIntersects(biggrid,grid250,byid=T)
key <- unlist(lapply(apply(key,1,which),'[[',1)) #remove the ones that stradle the border

# plot and example to use the key
plot(biggrid[19,])
plot(grid250[key==19,],add=T)
```

Use the key to set determine to which `biggrid` cell the sites correspond.
```{r translate sites to new grid}
dataset$grid0 <- key[dataset$site0]
dataset$grid <- key[dataset$site]
head(dataset)
```
## Calculate connectivity
```{r frequency}
# caculate a frequency table
con_mat_table <- table(factor(dataset$grid0,levels=1:length(biggrid)),
                     factor(dataset$grid,levels=1:length(biggrid))
)

# I find the `table` objects in R a bit awkward to handle, so you can examin it via a `data.frame`. In this format, `Var1` is equivalent to `grid0`; your source cells. While `Var2` is equivalent to `grid`; your settlement cells
con_mat_df <- as.data.frame(con_mat_table)
head(con_mat_df)

# ... or a matrix. In this format, the rownames are equivalent to `grid0`; your source cells. While the column names are equivalent to `grid`; your settlement cells
con_mat <- as.matrix(con_mat_table)
head(con_mat)
```

However, you'll notice that these are frequencies, not probabilities. First let's determine how many larvae were released from each cell
```{r calculate percents}
# caculate a frequency table
n <- as.data.frame(table(factor(dataset$grid0,levels=1:length(biggrid))))

# now we can divide the frequency by n to get probabilities. (what I did below only works if the levels of the factors in `n` and con_mat_df$Var1 are the same!)
con_mat_df$percent <- con_mat_df$Freq/n$Freq
head(con_mat_df)

# and of course we can do the same operation in matrix form.
con_mat_percent <- con_mat/n$Freq
head(con_mat_percent)
```
# Dispersal Kernels
Instead of calculating the distance between each grid cell, I'm going to represent the grid via row and column numbers. This will ignore the actual 'in water' distance travelled, but it is a lot quicker for demonstration purposes.

First, we'll extract the projected coordinates of the center of each cell.
```{r extract coordinates in meters}
biggrid$row <- round(coordinates(spTransform(biggrid,CRS(proj))))[,2]
biggrid$col <- round(coordinates(spTransform(biggrid,CRS(proj))))[,1]
```

Then, we can standardize those coordinates into row and column numbers. 
```{r standarize row & col}
rowsize <- sort(unique(biggrid$row))[2]-sort(unique(biggrid$row))[1]
biggrid$row <- round((biggrid$row-min(biggrid$row))/rowsize)

colsize <- sort(unique(biggrid$col))[2]-sort(unique(biggrid$col))[1]
biggrid$col <- round((biggrid$col-min(biggrid$col))/colsize)
```

Then, since we already know which grid cell each particle originated `grid0` and where each particle settled `grid`, we can now calculate the difference in column/row numbers. We multiply that difference by the `rowsize` or `colsize` to get the vertical (`NS`) distance and horizontal (`WE`) distance. Finally we can also plot the dispersal kernel.
```{r generate histogram}
dataset$NS <- (biggrid$row[dataset$grid]-biggrid$row[dataset$grid0])*rowsize
dataset$WE <- (biggrid$col[dataset$grid]-biggrid$col[dataset$grid0])*colsize

hist(dataset$NS, col='#d95f0280',
     breaks=seq(-1.5*10^6,1.5*10^6,10^5),
     xlim=c(
         min(c(dataset$NS,dataset$WE),na.rm=T),
         max(c(dataset$NS,dataset$WE),na.rm=T)
     )
)
hist(dataset$WE, col='#7570b380', add=T,
     breaks=seq(-1.5*10^6,1.5*10^6,10^5)
)
```

